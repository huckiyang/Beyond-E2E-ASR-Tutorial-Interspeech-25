# 📜 Beyond End-to-End ASR: Integrating Long-Context Acoustic and Linguistic Insights  

[![Conference](https://img.shields.io/badge/Event-ICASSP_2025-blue)](https://2025.ieeeicassp.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/Paper-PDF-red)](link-to-paper.pdf)
[![Slides](https://img.shields.io/badge/Slides-Available-orange)](link-to-slides.pdf)

> **Taejin Park** (NVIDIA) · **Huck Yang** (NVIDIA) · **Kyu Han** (AWS) · **Shinji Watanabe** (CMU)

---

## 📌 Overview  

Modern **automatic speech recognition (ASR)** excels in *data-rich* languages like English — yet often fails for **low-resource languages**, **accents**, **multi-speaker conversations**, or **long-context scenarios**.  

This tutorial introduces **long-context acoustic & linguistic modeling** to make ASR **fairer, more inclusive, and more robust**.  

**You will learn:**
- ✅ How to evaluate **long-form ASR** with realistic benchmarks  
- ✅ Acoustic & semantic **context modeling techniques**  
- ✅ **Multi-speaker processing** & **speech-LLM integration**  
- ✅ **RAG-based error correction** for transcription  
- ✅ Benchmarks for noisy, multilingual, and extended-speech scenarios  

---

## 🧠 Key Topics  

| Theme | Description |
|-------|-------------|
| **Fairness & Inclusivity** | Tackling ASR bias in underrepresented communities |
| **Long-Context Processing** | Retaining acoustic & linguistic cues over extended speech |
| **Multi-Speaker & Diarization** | Handling overlapping and multi-talker audio |
| **Speech-LLM Integration** | Merging ASR with large language models |
| **RAG-Based Repair** | Retrieval-augmented transcription correction |

---

## 📊 Benchmarks & Datasets  

We evaluate using:  

- **CHiME** – Real-world noisy speech  
- **LibriHeavy** – Long-form audiobook speech  
- **SLUE** – Spoken language understanding evaluation  
- **Speech QA** – Spoken question answering  
- **Dynamic SUPERB** – Comprehensive speech benchmark with long-context tests  

---

## 🛠 Evaluation Metrics  

- 🎯 **Multi-talker diarization** – Who spoke when  
- 🧩 **Semantic evaluation** – Capturing meaning beyond words  
- 📚 **RAG-based repair** – Retrieval-based error fixing  
- 🗣 **Speaker variability resilience** – Accents, dialects, and code-switching  

---

## 🔍 Why Long-Context ASR?  

Long-context ASR enables:  
- 🗨 **Conversation tracking** over extended dialogues  
- 🌏 **Low-resource language** support  
- 🔄 **Code-switching** handling  
- 🤝 **Accessibility** for marginalized communities  

---

## 👩‍🔬 Organizers  

### **Taejin Park** — *Senior Research Scientist, NVIDIA*  
Deep learning for speech processing, context-aware diarization, multi-speaker ASR.  
Ph.D. USC, former researcher at ETRI, internships at Microsoft, Amazon Alexa, Capio Inc.  

### **Huck Yang** — *Senior Research Scientist, NVIDIA*  
Speech-language modeling, robust ASR, multi-modal alignment.  
Ph.D. Georgia Tech, area chair for ICASSP, EMNLP, SLT, NAACL.  

### **Kyu Han** — *Senior Director, Oracle Cloud Infrastructure*  
Speech & language tech leader at AWS, IBM, Ford, Capio.ai, JD.com, ASAPP.  
ISCA Best Paper Award (2018). IEEE SPS Technical Committee member.  

### **Shinji Watanabe** — *Associate Professor, CMU*  
ASR, speech enhancement, SLU, ML for speech.  
500+ papers, IEEE & ISCA Fellow, ISCA Interspeech Best Paper Award (2024).  

---

## 📅 Tutorial Schedule  

1. **Introduction to Long-Context ASR Challenges**  
2. **Datasets & Benchmarks**  
3. **Acoustic Context Modeling**  
4. **Linguistic & Semantic Context Modeling**  
5. **Multi-Speaker Processing & Diarization**  
6. **Speech-LLM Integration**  
7. **RAG-based Error Correction**  
8. **Future Directions for Fair & Inclusive ASR**  

---

## 📚 Resources  

- [CHiME Challenge](http://spandh.dcs.shef.ac.uk/chime_challenge/)  
- LibriHeavy Dataset  
- SLUE Benchmark  
- Speech QA Dataset  
- Dynamic SUPERB Benchmark  

---

> 💡 **Key Takeaway:** Integrating *long-context acoustic modeling*, *linguistic understanding*, and *LLM reasoning* moves ASR **beyond transcription** toward **inclusive, fair, and context-aware speech technologies**.



